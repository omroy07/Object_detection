# ğŸ§  YOLO-Based Room & Object Detection

## ğŸ“Œ Project Overview
This project uses **YOLO (You Only Look Once)** or **OpenCV** to detect objects within a room from video or image input. The goal is to enhance real-time user interaction by recognizing key objects (e.g., chairs, laptops, bottles) and optionally triggering **AI-based actions** based on the detected objects.

---

## ğŸ¯ Features

- Real-time object detection using YOLOv5 or OpenCV DNN
- Object labeling with bounding boxes and confidence scores
- Optional voice or chatbot prompts triggered by specific detections
- Automation-ready (e.g., turn on a device when an object is detected)

---

## ğŸ”§ Setup Instructions

### 1. Clone the Repository
```bash
git clone https://github.com/yourusername/yolo-room-detection.git
cd yolo-room-detection
```
### 2. Install Dependencies

Make sure **Python 3.8+** is installed on your system.

Install the required Python packages:

```bash
pip install -r requirements.txt
```
### 3. Download YOLOv5 Model

Download the pre-trained YOLOv5 weights (e.g., `yolov5s.pt`) from the official [YOLOv5 GitHub Repository](https://github.com/ultralytics/yolov5).

Place the `.pt` file into the `models/` directory of this project:

```bash
mkdir -p models
mv yolov5s.pt models/
```
## ğŸ“Œ Future Enhancements

- ğŸ™ï¸ **Voice Command Integration**  
  Allow users to control the system or respond to detected objects using voice input.

- ğŸ  **Multi-Room Detection Logic**  
  Detect and differentiate between multiple rooms based on visual context or metadata.

- ğŸ‘¤ **User-Specific Object Personalization**  
  Train the system to recognize user-specific objects or preferences for a tailored experience.

- ğŸŒ **Home Automation API Integration**  
  Integrate with platforms like **Home Assistant** to trigger real-world actions based on object detection.
